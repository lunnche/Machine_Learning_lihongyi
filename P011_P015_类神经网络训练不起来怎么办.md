## 局部最小值与鞍点

为什么optimization会fail？

你经常会发现，随着你的参数不断update，你的training的loss不会再下降，但是你对这个loss还是不满意，  
有时候你甚至发现你的model一开始就train不起来，不管你怎么update你的参数，你的loss始终掉不下去，  

过去常有的一个猜想是，走到一个地方，这个地方参数对loss的微分为0，此时，gradient descent没有办法再update参数

常有人说，做deep learning 用gradient descent ，你会卡在local minima  

但你写paper的时候，千万不要说什么卡在local minima这种事情，别人会觉得你非常没有水准，  

为什么？
因为不是只有local minima的gradient为0，还有其他情况，比如saddle point  

saddle point 就是 gradient 是0 ，但不是local minima  ,也不是local maxima的地方，前后比它高，左右比它低。  

gradient为0的点，统称为critical point ,所以呢你可以说你的gradient没有办法再下降，也许是因为卡在了critical point。但你不能说卡在local minima。


问题：你卡在了某个critical point ，如何区分是local minima 还是 saddle point?  

为什么我想要知道这一点？
因为如果卡在local minima，那就是没路可走，saddle point旁边还是有路可走的，还是可以让你的loss更低的，

![image-20220214092750643](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214092750643.png)

怎么区分，用到一点数学  

## Tayler series Approximation

虽然我们没办法完整知道loss 函数长什么样，但如果给定一组参数,在它附近的loss func    tion是有办法被写出来的。

$L(\theta)$ around $\theta = \theta'$ can be approximated below 

$$
\LARGE
(\theta) \approx L({\color{blue}\theta'}) + (\theta - {\theta}')^T {\color{green}g}+\frac{1}{2}(\theta - {\color{blue}\theta'})^T {\color{red}H} (\theta - {\color{blue}\theta'})
$$

<font size="5">Gradient <font color="green">g</font> is a vector</font>
$$
\LARGE
{\color{green}g}=\nabla L({\color{blue}\theta'})\ \ \ \ 
{\color{green}g_i}=\frac{\partial{L({\color{blue}\theta'})}}{\partial{\theta_i}}
$$

$L(\theta)$这个loss function，这个error surface在$\theta'$附近，可以用后边那两项相加来近似，第一项是绿虚线，第二项是红虚线

![image-20220214102634418](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214102634418.png)

![image-20220214102657034](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214102657034.png)



![image-20220214102714521](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214102714521.png)

g是一次微分，H里面有二次微分的项







g是一个向量，$g_i$就是g的第i个component

在上面红色H：海塞矩阵  

Hessian <font size="8" color="red">H</font> is a <u>matrix</u>
$$
\LARGE
{\color{red}H}_{ij}=\frac{\partial^2}{\partial{\theta_i}\partial{\theta_j}} L({\color{blue}\theta'})
$$

H里面放的是L的二次微分，第i个row，第j个column的值，就是$L(\theta')$先对$\theta_i$做微分，再对$\theta_j$做微分，（次序不影响二阶导数?）做两次微分之后的结果就是$H_{ij}$


如果我们今天走到一个critical point，那意味着gradient为0，即绿色项为0，只剩下红色项，根据红色项来判断$\theta'$附近的error surface长什么样子，进而可以判断该点是local min,local max,or saddle point。

![image-20220214103427261](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214103427261.png)

来看看怎么根据红色项来判断$\theta'$附近的状况，

![image-20220214211032564](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214211032564.png)

![image-20220214211412007](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214211412007.png)

![image-20220214211844470](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214211844470.png)


H不但能告诉我们是否saddle point，还may tell us parameter update direction.  


![image-20220214212019975](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214212019975.png)

![image-20220214213441900](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214213441900.png)

回到刚才那个栗子🌰

![image-20220214215928644](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214215928644.png)

## Saddle point v.s. local minima  
到P11 25：35

saddle point 和 local minima 谁更常见？  

![image-20220215093038482](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215093038482.png)

![image-20220215093232064](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215093232064.png)

![image-20220215093457956](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215093457956.png)

在三维空间中无路可走的情况，在高维空间是有路可走的，error surface会不会也一样？

比如在二维空间中是个local minima，会不会在三维空间中只是个saddle point，
会不会在二维的空间中我们无路可走，在更高维的空间中其实有路可以走，维度越高可以走的路就越多？
今天我们再训练一个network的时候，我们的参数动辄百万，千万，上亿，所以我们的error surface其实是在一个非常高的维度中，我们的参数有多少，就代表我们error surface的维度有多少，有一千万个参数，network的维度就是1000万，维度这么高，会不会其实有很多路可以走呢？如果路很多，会不会其实local minima本来就很少呢？

![image-20220215101402567](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215101402567.png)

经验上来说，如果你实做neural network，会发现支持上述论点。  

下图可见，实做中，local minima很少，你往往卡在了saddle point  

![image-20220215102013652](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215102013652.png)

你看到图里 下边 minimum ratio 0.5处一堆点   说明啥   说明实做中，极端情况下，也还有一半的case特征值是负的，有一半的路其实都还可以让loss下降，经验上来说，local minima真的很少，多数时候，你train到一个地方，你的gradient真的很小，你的参数不再update了，往往是卡在了saddle point。

下面来看卡在saddle point附近非常平坦的地方，有什么解决方法？

![image-20220215104019675](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215104019675.png)


