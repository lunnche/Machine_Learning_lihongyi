## 局部最小值与鞍点

为什么optimization会fail？

你经常会发现，随着你的参数不断update，你的training的loss不会再下降，但是你对这个loss还是不满意，  
有时候你甚至发现你的model一开始就train不起来，不管你怎么update你的参数，你的loss始终掉不下去，  

过去常有的一个猜想是，走到一个地方，这个地方参数对loss的微分为0，此时，gradient descent没有办法再update参数

常有人说，做deep learning 用gradient descent ，你会卡在local minima  

但你写paper的时候，千万不要说什么卡在local minima这种事情，别人会觉得你非常没有水准，  

为什么？
因为不是只有local minima的gradient为0，还有其他情况，比如saddle point  

saddle point 就是 gradient 是0 ，但不是local minima  ,也不是local maxima的地方，前后比它高，左右比它低。  

gradient为0的点，统称为critical point ,所以呢你可以说你的gradient没有办法再下降，也许是因为卡在了critical point。但你不能说卡在local minima。


问题：你卡在了某个critical point ，如何区分是local minima 还是 saddle point?  

为什么我想要知道这一点？
因为如果卡在local minima，那就是没路可走，saddle point旁边还是有路可走的，还是可以让你的loss更低的，

![image-20220214092750643](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214092750643.png)

怎么区分，用到一点数学  

## Tayler series Approximation

虽然我们没办法完整知道loss 函数长什么样，但如果给定一组参数,在它附近的loss func    tion是有办法被写出来的。

$L(\theta)$ around $\theta = \theta'$ can be approximated below 

$$
\LARGE
(\theta) \approx L({\color{blue}\theta'}) + (\theta - {\theta}')^T {\color{green}g}+\frac{1}{2}(\theta - {\color{blue}\theta'})^T {\color{red}H} (\theta - {\color{blue}\theta'})
$$

<font size="5">Gradient <font color="green">g</font> is a vector</font>
$$
\LARGE
{\color{green}g}=\nabla L({\color{blue}\theta'})\ \ \ \ 
{\color{green}g_i}=\frac{\partial{L({\color{blue}\theta'})}}{\partial{\theta_i}}
$$

$L(\theta)$这个loss function，这个error surface在$\theta'$附近，可以用后边那两项相加来近似，第一项是绿虚线，第二项是红虚线

![image-20220214102634418](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214102634418.png)

![image-20220214102657034](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214102657034.png)



![image-20220214102714521](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214102714521.png)

g是一次微分，H里面有二次微分的项







g是一个向量，$g_i$就是g的第i个component

在上面红色H：海塞矩阵  

Hessian <font size="8" color="red">H</font> is a <u>matrix</u>
$$
\LARGE
{\color{red}H}_{ij}=\frac{\partial^2}{\partial{\theta_i}\partial{\theta_j}} L({\color{blue}\theta'})
$$

H里面放的是L的二次微分，第i个row，第j个column的值，就是$L(\theta')$先对$\theta_i$做微分，再对$\theta_j$做微分，（次序不影响二阶导数?）做两次微分之后的结果就是$H_{ij}$


如果我们今天走到一个critical point，那意味着gradient为0，即绿色项为0，只剩下红色项，根据红色项来判断$\theta'$附近的error surface长什么样子，进而可以判断该点是local min,local max,or saddle point。

![image-20220214103427261](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214103427261.png)

来看看怎么根据红色项来判断$\theta'$附近的状况，

![image-20220214211032564](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214211032564.png)

![image-20220214211412007](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214211412007.png)

![image-20220214211844470](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214211844470.png)


H不但能告诉我们是否saddle point，还may tell us parameter update direction.  


![image-20220214212019975](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214212019975.png)

![image-20220214213441900](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214213441900.png)

回到刚才那个栗子🌰

![image-20220214215928644](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214215928644.png)

## Saddle point v.s. local minima  
到P11 25：35

saddle point 和 local minima 谁更常见？  

![image-20220215093038482](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215093038482.png)

![image-20220215093232064](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215093232064.png)

![image-20220215093457956](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215093457956.png)

在三维空间中无路可走的情况，在高维空间是有路可走的，error surface会不会也一样？

比如在二维空间中是个local minima，会不会在三维空间中只是个saddle point，
会不会在二维的空间中我们无路可走，在更高维的空间中其实有路可以走，维度越高可以走的路就越多？
今天我们再训练一个network的时候，我们的参数动辄百万，千万，上亿，所以我们的error surface其实是在一个非常高的维度中，我们的参数有多少，就代表我们error surface的维度有多少，有一千万个参数，network的维度就是1000万，维度这么高，会不会其实有很多路可以走呢？如果路很多，会不会其实local minima本来就很少呢？

![image-20220215101402567](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215101402567.png)

经验上来说，如果你实做neural network，会发现支持上述论点。  

下图可见，实做中，local minima很少，你往往卡在了saddle point  

![image-20220215102013652](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215102013652.png)

你看到图里 下边 minimum ratio 0.5处一堆点   说明啥   说明实做中，极端情况下，也还有一半的case特征值是负的，有一半的路其实都还可以让loss下降，经验上来说，local minima真的很少，多数时候，你train到一个地方，你的gradient真的很小，你的参数不再update了，往往是卡在了saddle point。

下面来看卡在saddle point附近非常平坦的地方，有什么解决方法？

![image-20220215104019675](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215104019675.png)

## Tips for training
Batch and Momentum

## Batch
为啥训练中要用batch
实际在算微分的时候，不是对所有data算出来的loss做微分，而是把所有data分成一个个batch（mini batch)，每个batch的大小是B,每次在update参数的时候，是拿一个B这么多资料出来，算个loss，算个gradient，update参数，再拿另一笔B这么多资料，  
我们不会拿所有资料一起去算出Loss，只会拿一个batch的资料，  
所有的batch看过一遍叫做一个epoch，事实上，你在把所有资料分成一个个batch的时候，你会做一件事情叫做Shuffle，  
Shuffle有很多做法，一个常见做法就是：
在每一个epoch开始之前，会分一次batch，每一个epoch的batch都不一样，哪些资料在同一个epoch里面，每个batch都不同，  

![image-20220215140728963](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215140728963.png)

## Small Batch v.s. Large Batch  
为啥要用batch？  
比较左右两个case，假设有20笔训练资料，左边的case就是没有用batch（或者说我的batch设置得和我的训练资料一样大，这种状况叫做full batch），右边用了，batch size=1 ，这是两个最极端的情况，  
看左边，因为没有用batch，我们有20笔训练资料，我们的model必须把20笔资料都看完，才能够计算Loss，才能够计算gradient，  
如果batch size=1的话呢，代表我们每次update参数的时候，只要看一笔资料就好了，在一个epoch里面，update20次参数，  
用一笔资料算出来的loss显然是比较noisy的，所有会发现这20个update的方向曲曲折折的，左边和右边哪一个比较好呢？  
左边这种方式蓄力比较长，技能冷却时间比较长，但是走得比较稳，  
实际上，考虑平行运算的话，左边不一定时间长，来看实验结果，  
事实上，比较大的Batch Size，你要算Loss，再进而算gradient，所需要的时间，不一定比小的batch size要花的时间长，  
下面这个是MNIST（手写数字辨识）的栗子，MNIST是机器学习的果蝇，

![image-20220215142928308](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215142928308.png)

注意最上面那些链接，可以看到时代的演进，以前GPU需要算几分钟的，现在10秒搞定，  

![image-20220215143347382](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215143347382.png)

![image-20220215143605067](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215143605067.png)

看到这里，发现既然大的batch size在并行计算时并不比小batch size的慢，而且小batch size的需要花更多的时间去see全部的数据，那么就可以说大的batch size比较好吗？不是  
发现：
noisy的gradient反而可以帮助training，下图可看到，batch size越大，validation set上的效果越差。  
这个不是overfitting，因为在trainning set 和validation set上都差。  
这个也不是model bias，因为是同样的模型。  

![image-20220215145006676](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215145006676.png)

为什么会这样子呢？  
因为small batch size更容易跳出局部最优  
small batch情况下，每个batch你用的loss function都是略有差异的，

![image-20220215145726607](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215145726607.png)

另一个神奇的地方是，small batch对testing data也有帮助。  
假设你通过一些方法（比如调整learning rate)把large batch调得和small batch在training accuracy上一样好，那么在testing accuracy上，还是small batch更好些。  
large batch更容易overfitting  

好minima和坏minima
如果一个minima在一个峡谷里面，就认为它是坏minima，在平原上，就是好的minima
small batch容易跳出峡谷，当然这只是一种解释，不是所有人都相信。  

![image-20220215152228121](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215152228121.png)

![image-20220215152539064](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215152539064.png)

最后那行generalization就是指在testing data上的accuracy比较好。  

有没有可能鱼与熊掌兼得呢？有可能，有很多这方面研究：  

![image-20220215152829747](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215152829747.png)
## Momentum  
动量梯度下降
另一个可以对抗saddle point 和 local minima的技术，  
它的概念呢就是想象error suface就是真正的斜坡，而我们的参数是一个球，把球从斜坡上滚下来，如果是gradient descent，它走到critical point就停住了，

![image-20220215153337427](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215153337427.png)

## (Vanilla) Gradient Descent
复习下原来的gradient descent什么样？  
vanilla意思就是一般的  

![image-20220215153746658](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215153746658.png)

## Gradient Descent + Momentum  
每次移动参数的时候，我们不是只往gradient的反方向移动参数，现在呢是gradient的反方向加上前一步移动的方向，

![image-20220215154336227](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215154336227.png)

m可以写成之前算出来的所有gradient的weighted sum。   

加上momentum后，两种解读
1. gradient的反方向加上前一次移动方向，
2. 考虑过去所有gradient 的总和。  

![image-20220215154735413](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215154735413.png)

一个更简单的栗子：

![image-20220215155022151](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215155022151.png)

## Concluding Remarks
* Critical points have zero gradients.  
* Critical points can be either saddle points or local minima.  
    * Can be determined by the Hessian matrix.  
    * It is possible to escape saddle points along the derection of eigenvectors of the Hessian matrix.
    * Local minima may be rare.
* smaller batch size and momentum help escape critical points.  

critical point 不是你在训练network时面临的最大障碍：
## Error surface is rugged ..
Tips for training :Adaptive Learning Rate  
即给每一个参数不同的learning rate  

在训练一个network的时候，会把loss记录下来，你的loss原来很大，横轴代表你参数update的次数，随着update次数增大，loss会越来越小，最后就卡住，这时大家就会猜是否走到了critical point，在这个栗子中，loss不再下降的时候，gradient并没有变得很小，

![image-20220215160314184](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215160314184.png)

所以如果我们实做中，很少卡到local minima或者saddle point，那之前那张图是怎么做出来的呢？

要想把接近critical point，用一般的gradient descent其实是做不到的。  
一般实做中，你经历的大多数情况是，在gradient还很大的时候，你的loss就掉不下去了。  
你真正要怪罪的不是critical point而是其他原因。  

![image-20220215160813554](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215160813554.png)

如果不是因为critical point，为啥training会卡住呢，看个简单栗子：  

![image-20220215161728525](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220215161728525.png)

对于分厂复杂的error surface，你要train 一个deep neural network，gradient descent是你唯一可以依赖的工具，这个例子中凸优化有更好的减法，但你终归会遇到只能用gradient desent来解决的问题。但gradient descent连这么简单的error suface都做不好，所以我们需要更好的gradient descent的版本。  

到P13 09：32

## Different parameters needs different learning rate
怎么样客制化learning rate呢？不同的参数需要什么样的learning rate呢？
刚才的栗子中能看到一个大原则：如果在某一个方向上，gradient的值很小，非常的平坦，那我们就希望learning rate大一点，如果在某个方向上坡度很大，很陡峭，我们就希望learning rate小一点，
那么learning rate要如何自动地根据gradient的大小而调整呢？
我们需要改一下gradient descent原来的式子：
以前是讲所有参数的情况，这里问简化问题，先只看一个参数的情况

Formulation for **one** parameter:
$$
原来的式子
\LARGE
\theta_i^{t+1} \leftarrow \theta_i^t - \eta g_i^t\\
\LARGE
g_i^t = \frac{\partial{L}}{\partial{\theta_i}}|_{\theta=\theta^t}
$$
t是iteration的次数

现在改成：
$$
\LARGE
\theta_i^{t+1} \leftarrow \theta_i^t - \frac{\eta}{\sigma_i^t}g_i^t
$$

上式中
$$
\LARGE
\frac{\eta}{\sigma_i^t}
$$


![image-20220216085147539](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220216085147539.png)

是Parameter dependent的,下面来看看它有什么常见的计算方式：
290 思路就是：跳坑时每一步的距离，根据这个方向上的坡度做同向调整【弹幕】

Root Mean Square
$$
\LARGE
\theta_i^{t+1} \leftarrow \theta_i^t - \frac{\eta}{\sigma_i^t}g_i^t\ \ \ \sigma_i^t=\sqrt{\frac{1}{t+1}\sum_{i=0}^t(g_i^t)^2 }
$$

![image-20220216090656474](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220216090656474.png)

上面这招被用在一个叫Adagrad的方法里面。  

为啥这招能做到坡度大时候lerning rate减小，平坦的时候，learning rate增大呢

![image-20220216091048467](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220216091048467.png)

上述方法并不是终极版本，
它还有问题：就算是同一个参数，它需要的learning rate也会随时间改变，上述方法同一个参数，对应同样的learning rate。
看下面这个新月型号的error surface  
对于同一个参数，我们也期待不同的learning rate  

![image-20220216091526423](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220216091526423.png)

RMSProp  这个方法有点传奇，它找不到论文。

到P13 21：22
