## 局部最小值与鞍点

为什么optimization会fail？

你经常会发现，随着你的参数不断update，你的training的loss不会再下降，但是你对这个loss还是不满意，  
有时候你甚至发现你的model一开始就train不起来，不管你怎么update你的参数，你的loss始终掉不下去，  

过去常有的一个猜想是，走到一个地方，这个地方参数对loss的微分为0，此时，gradient descent没有办法再update参数

常有人说，做deep learning 用gradient descent ，你会卡在local minima  

但你写paper的时候，千万不要说什么卡在local minima这种事情，别人会觉得你非常没有水准，  

为什么？
因为不是只有local minima的gradient为0，还有其他情况，比如saddle point  

saddle point 就是 gradient 是0 ，但不是local minima  ,也不是local maxima的地方，前后比它高，左右比它低。  

gradient为0的点，统称为critical point ,所以呢你可以说你的gradient没有办法再下降，也许是因为卡在了critical point。但你不能说卡在local minima。


问题：你卡在了某个critical point ，如何区分是local minima 还是 saddle point?  

为什么我想要知道这一点？
因为如果卡在local minima，那就是没路可走，saddle point旁边还是有路可走的，还是可以让你的loss更低的，

![image-20220214092750643](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214092750643.png)

怎么区分，用到一点数学  

## Tayler series Approximation

虽然我们没办法完整知道loss 函数长什么样，但如果给定一组参数,在它附近的loss func    tion是有办法被写出来的。

$L(\theta)$ around $\theta = \theta'$ can be approximated below 

$$
\LARGE
(\theta) \approx L({\color{blue}\theta'}) + (\theta - {\theta}')^T {\color{green}g}+\frac{1}{2}(\theta - {\color{blue}\theta'})^T {\color{red}H} (\theta - {\color{blue}\theta'})
$$

<font size="5">Gradient <font color="green">g</font> is a vector</font>
$$
\LARGE
{\color{green}g}=\nabla L({\color{blue}\theta'})\ \ \ \ 
{\color{green}g_i}=\frac{\partial{L({\color{blue}\theta'})}}{\partial{\theta_i}}
$$

$L(\theta)$这个loss function，这个error surface在$\theta'$附近，可以用后边那两项相加来近似，第一项是绿虚线，第二项是红虚线

![image-20220214102634418](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214102634418.png)

![image-20220214102657034](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214102657034.png)



![image-20220214102714521](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214102714521.png)

g是一次微分，H里面有二次微分的项







g是一个向量，$g_i$就是g的第i个component

在上面红色H：海塞矩阵  

Hessian <font size="8" color="red">H</font> is a <u>matrix</u>
$$
\LARGE
{\color{red}H}_{ij}=\frac{\partial^2}{\partial{\theta_i}\partial{\theta_j}} L({\color{blue}\theta'})
$$

H里面放的是L的二次微分，第i个row，第j个column的值，就是$L(\theta')$先对$\theta_i$做微分，再对$\theta_j$做微分，（次序不影响二阶导数?）做两次微分之后的结果就是$H_{ij}$


如果我们今天走到一个critical point，那意味着gradient为0，即绿色项为0，只剩下红色项，根据红色项来判断$\theta'$附近的error surface长什么样子，进而可以判断该点是local min,local max,or saddle point。

![image-20220214103427261](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214103427261.png)

来看看怎么根据红色项来判断$\theta'$附近的状况，

![image-20220214211032564](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214211032564.png)

![image-20220214211412007](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214211412007.png)

![image-20220214211844470](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214211844470.png)


H不但能告诉我们是否saddle point，还may tell us parameter update direction.  


![image-20220214212019975](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214212019975.png)

![image-20220214213441900](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214213441900.png)

回到刚才那个栗子🌰

![image-20220214215928644](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220214215928644.png)

## Saddle point v.s. local minima  
到P11 25：35
